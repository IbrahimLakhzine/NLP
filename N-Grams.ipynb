{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd4dc6718ea0b7b66da33b181c834c83",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>. If this does not work there is a submission slot on LEA.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID of each team member into the variables. \n",
    "If you work alone please leave the second variable empty.\n",
    "'''\n",
    "member1 = ''\n",
    "member2 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10847ec7a2dab1b133200c3c83a346e1",
     "grade": false,
     "grade_id": "bigram_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "We want to build a count based bigram language model based on the book *Emma* by *Jane Austen*.\n",
    "\n",
    "In the next cell the book is read into the variables ```emma_sents``` and ```emma_words```.\n",
    "\n",
    "If the cell throws an error please execute the outcommented lines to download the data needed. This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]\n",
      "I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/ilakhz2s/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ilakhz2s/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "emma_sents = gutenberg.sents('austen-emma.txt')\n",
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "\n",
    "print(emma_words)\n",
    "print(emma_words[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "614e4ad5d34f9f3947907759d526496b",
     "grade": false,
     "grade_id": "statistics_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1) Statistics [4 Points]\n",
    "\n",
    "Please calculate the number of types and tokens for the book and save the values in the variables ```types``` and ```tokens```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63421c1642dea2a2f364ad97c5aabb9c",
     "grade": false,
     "grade_id": "statistics",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book Emma by Jane Austen consists of 192427 tokens and 7811 types.\n"
     ]
    }
   ],
   "source": [
    "types = 0\n",
    "tokens = 0\n",
    "past_token_str=['']\n",
    "tokens=len(emma_words)\n",
    "\n",
    "for i in range(0,tokens):\n",
    "    breaking=False\n",
    "    for str in past_token_str:\n",
    "        if emma_words[i]==str:\n",
    "            breaking=True\n",
    "            break\n",
    "            \n",
    "    if breaking==False:\n",
    "        past_token_str.append(emma_words[i])\n",
    "        types+=1\n",
    "            \n",
    "#raise NotImplementedError()\n",
    "\n",
    "print('The book Emma by Jane Austen consists of {} tokens and {} types.'.format(\n",
    "    tokens,\n",
    "    types\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff42cdad3a3f034939a5d099689b57b2",
     "grade": true,
     "grade_id": "test_statistics",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a38b6c5f76f5934e87f58860b22f707b",
     "grade": false,
     "grade_id": "heaps_law_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2) Heap's Law\n",
    "\n",
    "Let us validate Heap's law from the first chapter we read:\n",
    "\n",
    "$|V| = k * N^{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbf5b6abf6da41a1272104fb62e31270",
     "grade": false,
     "grade_id": "heaps_law_empirical_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2.1) Empirical Study [8 Points]\n",
    "\n",
    "We first want to plot the relationship between types and tokens for the book *Emma* by *Jane Austen*.\n",
    "\n",
    "For this you should fill the lists ```number_of_types``` and ```number_of_tokens``` with the corresponding values.\n",
    "\n",
    "So we want to investigate how many types we have after 1 token, 2 tokens, 3 tokens until we have read all the words from the book.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "- ```number_of_tokens```: ```[1, 2, 3, ..., 16, 17, 18, ...]```\n",
    "- ```number_of_types```:  ```[1, 2, 3, ..., 13, 14, 14, ...]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76b886d6a7d0ff2914a6e118c0c23d6c",
     "grade": false,
     "grade_id": "heaps_law_empirical",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "number_of_tokens = []\n",
    "number_of_types = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for i in [10, 100, 1000, 10000]:\n",
    "    print('After reading {} tokens we found {} types.'.format(\n",
    "        number_of_tokens[i - 1], number_of_types[i - 1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1ea21cd9e5e1d67914ce5ea8b5c1e10",
     "grade": true,
     "grade_id": "test_heaps_law_empirical",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17c2ff841f74b0232bd033a263a6529a",
     "grade": false,
     "grade_id": "empirical_plot_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2.2) Plot [10 Points]\n",
    "\n",
    "Please plot your findings:\n",
    "\n",
    "- x-Axis: Number of tokens\n",
    "- y-Axis: Number of types\n",
    "\n",
    "Make sure your plot has a grid, a legend, a title and x- and y-label.\n",
    "\n",
    "Add the values for the three books by Jane Austen as points in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aac334850ad80fff09b416a81925f679",
     "grade": true,
     "grade_id": "empirical_plot",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ff5d360c11f9ec1e8fc9c0387163ec8",
     "grade": false,
     "grade_id": "parameter_estimation_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2.3) Estimate parameters $k$ and $\\beta$ [8 points]\n",
    "\n",
    "We want to estimate the parameters $k$ and $\\beta$ for Heap's law based on our book.\n",
    "\n",
    "Use the function ```curve_fit``` from ```scipy.optimize``` with the previously calculated lists. Save your solution in the variables ```k``` and ```beta```.\n",
    "\n",
    "```curve_fit``` takes in three arguments, the function that relatex x values to y values together with its parameters, the observed x-values and the observed y-values. It return popt (the optimal parameters) and pcov (how well they fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fb940e2d3b52652a1d70cf112e30d4d",
     "grade": false,
     "grade_id": "parameter_estimation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, k, beta):\n",
    "    return k * x**beta\n",
    "\n",
    "k = 0\n",
    "beta = 0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('For the book Emma estimate k = {:.2f} and beta = {:.2f}'.format(\n",
    "    k,\n",
    "    beta\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc837c268400ad5c2cf2d99413a4694b",
     "grade": true,
     "grade_id": "test_parameter_estimation",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2880c74ddd4f1ce1736af9c8c66730f1",
     "grade": false,
     "grade_id": "combined_plot_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2.4) Combined plot [10 Points]\n",
    "\n",
    "In our Gutenberg corpus we have the following books by Jane Austen:\n",
    "\n",
    "- Emma ```gutenberg.words('austen-emma.txt')```\n",
    "- Sense ```gutenberg.words('austen-sense.txt')```\n",
    "- Persuasion ```gutenberg.words('austen-persuasion.txt')```\n",
    "\n",
    "Plot the number of types and tokens for each book as a point (total number of types and tokens) together with the function $|V| = k N^{\\beta}$ with your estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f19144642b5132172a465fee534b164e",
     "grade": true,
     "grade_id": "combined_plot",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee9b8e456e331989f25356df2231ed0c",
     "grade": false,
     "grade_id": "bigram_model_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1) Bigram Model [50 Points]\n",
    "\n",
    "We now want to build a bigram language model from the book *Emma* by *Jane Austen*.\n",
    "\n",
    "For this you should use the class given in the next cell.\n",
    "\n",
    "1. Read in the sentences from the book *Emma* by *Jane Austen*\n",
    "2. Write a method that returns the unigram count\n",
    "3. Write a method that returns the unigram probability\n",
    "4. Write a method that returns the bigram count\n",
    "5. Write a method that returns the bigram probability\n",
    "6. Write a method that returns the sentence probability based on bigrams\n",
    "\n",
    "*Hints:*\n",
    "\n",
    "- The next cell gives you some inspiration on how to implement the counting of bigrams\n",
    "- Everything should be precomputed in the constructor (```__init__```) and the other functions should not recount anything. If implemented efficiently all the computation will be done in a few seconds (less than 10)!\n",
    "- This class should be **self-contained** and not depend on any code from previous cells!\n",
    "- The ```window``` function is a memory friendly iterator over a list that gives you all n-grams from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cd12e6c6e73e0dc28f3bfbb103d7072",
     "grade": false,
     "grade_id": "bigram_model_example",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Example (you do not need to edit this cell):\n",
    "\n",
    "Suppose you have a very small corpus consisting of only the\n",
    "four unique words 'I', 'have', 'a', 'dog' and \n",
    "the sentence start and end markers '<s>' and '</s>'\n",
    "\n",
    "The corpus has the three sentences\n",
    "- <s> I have a dog </s>\n",
    "- <s> a dog I have </s>\n",
    "- <s> a dog </s>\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# First we define the index for each word (the order does not matter)\n",
    "index = {\n",
    "    'I': 0,\n",
    "    'have': 1,\n",
    "    'a': 2,\n",
    "    'dog': 3,\n",
    "    '<s>': 4,\n",
    "    '</s>': 5\n",
    "}\n",
    "\n",
    "# These are our bigrams from the sentences\n",
    "bigrams = [('<s>', 'I'), ('I', 'have'), ('have', 'a'), ('a', 'dog'), ('dog', '</s>'),\n",
    "           ('</s>', '<s>'), ('<s>', 'a'), ('a', 'dog'), ('dog', 'I'), ('I', 'have'), ('have', '</s>'),\n",
    "           ('</s>', '<s>'), ('<s>', 'a'), ('a', 'dog'), ('dog', '</s>')]\n",
    "\n",
    "# Next we create a matrix for the bigram counts,\n",
    "# each entry is a 16 Bit unsigned integer (dtype=np.uint16)\n",
    "counts = np.zeros((len(index), len(index)), dtype=np.uint16)\n",
    "\n",
    "# Fill it with the counts\n",
    "for bigram in bigrams:\n",
    "    index_first_word = index[bigram[0]]\n",
    "    index_second_word = index[bigram[1]]\n",
    "    counts[index_first_word, index_second_word] += 1\n",
    "    \n",
    "# Print out count matrix\n",
    "print(counts)\n",
    "\n",
    "# Check the count for the bigram ('I', 'have'):\n",
    "print('The bigram (\"I\", \"have\") exists {} times.'.format(\n",
    "    counts[index['I'], index['have']]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4a20bcf7536b5e6c505b0559f02021e",
     "grade": false,
     "grade_id": "bigram_model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "class BigramModel:\n",
    "    \n",
    "    def __init__(self, sentences: List[List[str]]):\n",
    "        '''\n",
    "        Takes in a list of sentences, where each sentence is a \n",
    "        list of words.\n",
    "        \n",
    "        Arguments:\n",
    "            sentences -- List of lists of words (e.g. [['I', 'have', 'a', 'dog'],\n",
    "                                                       ['a', 'dog', 'I', 'have']])\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def window(self, seq, n=2):\n",
    "        '''\n",
    "        Returns a sliding window (of width n) over data from the iterable\n",
    "        \n",
    "        Arguments:\n",
    "            seq      -- the iterable (e.g. list, set, etc) to run the window over\n",
    "            n        -- the size of the window\n",
    "        Returns:\n",
    "            iterator -- an iterator over the sliding windows\n",
    "            \n",
    "        Usage:\n",
    "            my_list = [1, 2, 3, 4]\n",
    "            for slice in self.window(my_list):\n",
    "                print(slice)\n",
    "                \n",
    "            # Output: (1, 2)\n",
    "                      (2, 3)\n",
    "                      (3, 4)\n",
    "        '''\n",
    "        \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "        \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "        it = iter(seq)\n",
    "        result = tuple(islice(it, n))\n",
    "        if len(result) == n:\n",
    "            yield result\n",
    "        for elem in it:\n",
    "            result = result[1:] + (elem,)\n",
    "            yield result\n",
    "            \n",
    "    def unigram_count(self, word: str) -> int:\n",
    "        '''\n",
    "        Returns the unigram count for the word.\n",
    "        If the word does not exist in our corpus, return 0.\n",
    "        \n",
    "        Arguments:\n",
    "            word  -- word we want to know the count of\n",
    "        Returns:\n",
    "            count -- how often the word appears in the corpus\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def unigram_probability(self, word:str) -> float:\n",
    "        '''\n",
    "        Returns the unigram probability for the word.\n",
    "        If the word does not exist in our corpus, return 0.\n",
    "        \n",
    "        Arguments:\n",
    "            word        -- word we want to know the probability of\n",
    "        Returns:\n",
    "            probability -- how likely it is to choose the word at random\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def bigram_count(self, word1:str, word2:str) -> int:\n",
    "        '''\n",
    "        Returns the bigram count for the word1 followed by word2.\n",
    "        If either of the words does not exist in our corpus, return 0.\n",
    "        \n",
    "        Arguments:\n",
    "            word1  -- first word of the bigram\n",
    "            word2  -- second word of the bigram\n",
    "        Returns:\n",
    "            count  -- how often the bigram appears in the corpus\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def bigram_probability(self, word1:str, word2:str) -> float:\n",
    "        '''\n",
    "        Returns the bigram probability for the word1 followed by word2.\n",
    "        This is the conditional probability P(word2 | word1).\n",
    "        If either of the words does not exist in our corpus, return 0.\n",
    "        \n",
    "        Arguments:\n",
    "            word1       -- first word of the bigram\n",
    "            word2       -- second word of the bigram\n",
    "        Returns:\n",
    "            probability -- how likely it is to choose the word at random\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def sentence_probability(self, sentence:List[str]) -> float:\n",
    "        '''\n",
    "        Return the probability for the given sentence based on our\n",
    "        bigram probabilities\n",
    "        \n",
    "        Arguments:\n",
    "            sentence    -- list of tokens from the sentence \n",
    "                           (e.g. ['<s>', 'I', 'have', 'a', 'dog', '</s>'])\n",
    "        Returns:\n",
    "            probability -- probability of the sentence\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "          \n",
    "model = BigramModel(gutenberg.sents('austen-emma.txt'))\n",
    "\n",
    "# Some prints and tests\n",
    "\n",
    "print('The unigram \"I\" appears {} times in the book!'.format(\n",
    "    model.unigram_count('I')\n",
    ")) ## Should print 3178\n",
    "\n",
    "print('The probability for the unigram \"Emma\" is {:.4f}.'.format(\n",
    "    model.unigram_probability('Emma')\n",
    ")) ## Should print 0.0042\n",
    "\n",
    "print('The bigram \"I am\" appears {} times in the book!'.format(\n",
    "    model.bigram_count('I', 'am')\n",
    ")) ## Should print 395\n",
    "\n",
    "print('The probability for the bigram \"I have\" is {:.4f}.'.format(\n",
    "    model.bigram_probability('I', 'have')\n",
    ")) ## Should print 0.0884\n",
    "\n",
    "print('The sentence probability for the sentence \"What a horrible idea!\" is {:.4e}.'.format(\n",
    "    model.sentence_probability(['<s>', 'What', 'a', 'horrible', 'idea', '!', '</s>'])\n",
    ")) ## Should print 6.4835e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ad46378864a8433e31b3fac7e23103e",
     "grade": true,
     "grade_id": "test_bigram_model",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb222d009aed0d9a44473753ebac2ff8",
     "grade": false,
     "grade_id": "bigram_application_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2) Using the model [10 Points]\n",
    "\n",
    "With our model we can now answer some questions.\n",
    "\n",
    "1. How often does a certain word appear in the book?\n",
    "    - Give the number of times the word 'dog' appears in the book - Store this in the variable ```count_dog```.\n",
    "    - Give the number of times the word 'She' appears in the book - Store this in the variable ```count_She```.\n",
    "2. How many sentences start or end with a certain word or token?\n",
    "    - Give the probability that a sentence starts with the word 'I' - Store this in the variable ```p_sentence_begins_with_I```\n",
    "    - Give the probability that a sentence ends with '!' - Store this in the variable ```p_sentence_ends_in_exlamation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac95a73c38d812c5f54b0905eec6bb46",
     "grade": false,
     "grade_id": "bigram_application",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = BigramModel(gutenberg.sents('austen-emma.txt'))\n",
    "\n",
    "count_dog = 0\n",
    "count_She = 0\n",
    "p_sentence_begins_with_I = 0\n",
    "p_sentence_ends_in_exlamation = 0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('The word \"dog\" appears {} time(s) in the book.'.format(\n",
    "    count_dog\n",
    "))\n",
    "print('The word \"She\" appears {} time(s) in the book.'.format(\n",
    "    count_She\n",
    "))\n",
    "print('The probability that a sentence starts with \"I\" is {:.4f}'.format(\n",
    "    p_sentence_begins_with_I\n",
    "))\n",
    "print('The probability that a sentence ends in \"!\" is {:.4f}'.format(\n",
    "    p_sentence_ends_in_exlamation\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "759582645386c7f162a1f53102ce9781",
     "grade": true,
     "grade_id": "test_bigram_application",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
